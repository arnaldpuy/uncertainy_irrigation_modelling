---
title: "Uncertainty persists after 50 years of global irrigation modelling"
subtitle: "R code"
author: "Arnald Puy"
header-includes:
  - \usepackage[font=footnotesize]{caption}
  - \usepackage{dirtytalk}
  - \usepackage{booktabs}
  - \usepackage{tabulary}
  - \usepackage{enumitem}
  - \usepackage{lmodern}
  - \usepackage{amsmath}
  - \usepackage{mathtools}
  - \usepackage[T1]{fontenc}
  - \usepackage{tikz}
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
    keep_tex: true
  word_document:
    toc: no
    toc_depth: '2'
  html_document:
    keep_md: true
link-citations: yes
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev = "pdf", cache = TRUE)
```

\newpage

# Preliminary functions

```{r, warning=FALSE, message=FALSE}

#   PRELIMINARY FUNCTIONS ######################################################

sensobol::load_packages(c("openxlsx", "data.table", "tidyverse","cowplot", 
                          "benchmarkme", "parallel", "wesanderson", "scales", "ncdf4", 
                          "countrycode", "rworldmap", "sp", "doParallel", "here", "lme4", 
                          "microbenchmark", "mgcv", "brms", "randomForest", "here"))

# Create custom theme -----------------------------------------------------------

theme_AP <- function() {
  theme_bw() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          legend.background = element_rect(fill = "transparent",
                                           color = NA),
          legend.key = element_rect(fill = "transparent",
                                    color = NA), 
          strip.background = element_rect(fill = "white"), 
          legend.text = element_text(size = 7.3), 
          axis.title = element_text(size = 10),
          legend.key.width = unit(0.4, "cm"), 
          legend.key.height = unit(0.4, "cm"), 
          legend.key.spacing.y = unit(0, "lines"),
          legend.box.spacing = unit(0, "pt"),
          legend.title = element_text(size = 7.8), 
          axis.text.x = element_text(size = 7), 
          axis.text.y = element_text(size = 7), 
          axis.title.x = element_text(size = 7.3), 
          axis.title.y = element_text(size = 7.3),
          plot.title = element_text(size = 8),
          strip.text.x = element_text(size = 7.4), 
          strip.text.y = element_text(size = 7.4)) 
}

# Select color palette ----------------------------------------------------------

selected.palette <- "Darjeeling1"
```

```{r source_functions, warning=FALSE, message=FALSE, results="hide"}

# SOURCE ALL R FUNCTIONS NEEDED FOR THE STUDY ###################################

# Source all .R files in the "functions" folder --------------------------------

r_functions <- list.files(path = here("functions"), pattern = "\\.R$", full.names = TRUE)
lapply(r_functions, source)

```

# ISIMIP Data

## Historical data

```{r isimip_data}

# RETRIEVE DATA FROM ISIMIP ####################################################

# Create vector with list of files ---------------------------------------------

list.of.files <- list.files("./files/isimip")
model.names <- sub("^(.*?)_.*", "\\1", list.of.files)
climate.scenarios <- sapply(strsplit(list.of.files, "_"), function(x) x[2])
social.scenarios <- sapply(strsplit(list.of.files, "_"), function(x) x[which(x == "co2") - 1])
files.directory <- paste("./files/isimip", list.of.files, sep = "/")
start_year <- 1971

# Create parallel cluster -------------------------------------------------------

numCores <- detectCores() * 0.75
cl <- makeCluster(numCores)
registerDoParallel(cl)

# Run for loop -----------------------------------------------------------------

isimip.hist <- foreach(i = 1:length(files.directory),
                       .packages = c("data.table", "countrycode", "tidyverse",
                                "sp", "rworldmap", "ncdf4")) %dopar% {
                                  
                                  get_isimip_fun(nc_file = files.directory[i], 
                                                 variable = "airrww", 
                                                 start_year = start_year)
                                }

# Stop the cluster after the computation ---------------------------------------

stopCluster(cl)
```

```{r arrange_isimip_data, dependson="isimip_data"}

# ARRANGE DATA #################################################################

# Number of files --------------------------------------------------------------

list.of.files

# Name the slots ---------------------------------------------------------------

names(isimip.hist) <- paste(model.names, climate.scenarios, social.scenarios, sep = "/")

# Clean and bind dataset -------------------------------------------------------

isimip.dt <- rbindlist(isimip.hist, idcol = "model") %>%
  na.omit() %>%
  .[, model:= factor(model)] %>%
  .[, c("model", "climate", "social"):= tstrsplit(model, "/")]

fwrite(isimip.dt, "isimip.dt.csv")

# Pressoc: constant human impacts in the form of dams and reservoirs
# varsoc: variable human impacts.
```

### Plot data

```{r plot_isimip_dt_continent, dependson="arrange_isimip_data", fig.height=3.2}

# PLOT ISIMIP ##################################################################

# Continental level ------------------------------------------------------------

isimip.dt[, sum(V1, na.rm = TRUE), .(Continent, model, year, climate, social)] %>%
  ggplot(., aes(year, V1, group = interaction(climate, model), color = model, 
                linetype = climate)) +
  facet_wrap(social~Continent, scales = "free_y", ncol = 5) +
  geom_line() + 
  scale_x_continuous(breaks = breaks_pretty(n = 3)) +
  labs(x = "Year", y = bquote("IWW (km"^3 * ")"))  +
  theme_AP() +
  guides(color = guide_legend(nrow = 2)) +
  guides(linetype = guide_legend(nrow = 2)) +
  theme(legend.position = "top")
```

```{r plot_isimip_dt_global, dependson="arrange_isimip_data", fig.height=2.2, fig.width=3.7}

# Global level -----------------------------------------------------------------

isimip.dt[, sum(V1, na.rm = TRUE), .(year, model, climate, social)] %>%
  ggplot(., aes(year, V1, group = interaction(climate, model), color = model)) +
  geom_line() + 
  facet_wrap(~social) +
  labs(x = "Year", y = bquote("IWW (km"^3 * ")"))  +
  theme_AP() +
  theme(legend.position = "top")
```

## Predictions

```{r isimip_data_future}

# RETRIEVE PROJECTIONS FROM ISIMIP #############################################

# Create vector with list of files ---------------------------------------------

path.projections <- "./files/isimip_future"
list.of.files.projections <- list.files(path.projections)
files.directory.projections <- paste(path.projections, list.of.files.projections, sep = "/")
variable <- "airrww"
start_year <- 2006

# Create parallel cluster -------------------------------------------------------

numCores <- detectCores() * 0.75
cl <- makeCluster(numCores)
registerDoParallel(cl)

# Run for loop -----------------------------------------------------------------

isimip.future <- foreach(i = 1:length(files.directory.projections),
                       .packages = c("data.table", "countrycode", "tidyverse",
                                     "sp", "rworldmap", "ncdf4")) %dopar% {
                                       
                                       get_isimip_fun(nc_file = files.directory.projections[i], 
                                                      variable = variable, 
                                                      start_year = start_year)
                                     }

# Stop the cluster after the computation ---------------------------------------

stopCluster(cl)
```

```{r arrange_isimip_dt_future, dependson="isimip_data_future"}

# ARRANGE DATA #################################################################

# Number of files --------------------------------------------------------------

list.of.files.projections

# Arrange names ----------------------------------------------------------------

model.names <- sub("^(.*?)_.*", "\\1", list.of.files.projections)
pattern <- "ewembi_(.*?)soc"
climate <- sub(".*ewembi_(.*?)soc.*", "\\1", list.of.files.projections)
names(isimip.future) <- paste(model.names, climate, sep = "/")

# Clean and bind dataset -------------------------------------------------------

isimip.future.dt <- rbindlist(isimip.future, idcol = "model") %>%
  na.omit() %>%
  .[, model:= factor(model)] %>%
  .[, year:= as.numeric(year)] 

isimip.future.dt[, c("model", "climate") := tstrsplit(model, "/")]

# Export -----------------------------------------------------------------------

fwrite(isimip.future.dt, "isimip.future.dt.csv")
```

```{r plot_isimip_dt_future, dependson="arrange_isimip_dt_future"}

# PLOT ISIMIP ##################################################################

# Continental level ------------------------------------------------------------

isimip.future.dt[, sum(V1, na.rm = TRUE), .(year, Continent, model, climate)] %>%
  .[, climate:= gsub("_", "\\\\_", climate)] %>%
  ggplot(., aes(year, V1, group = climate, color = climate)) +
  facet_wrap(model~Continent, scales = "free_y", ncol = 5) +
  geom_line() +
  labs(x = "Year", y = bquote("IWW (km"^3 * ")"))  +
  theme_AP() +
  scale_x_continuous(breaks = breaks_pretty(n = 3)) +
  theme(legend.position = "top")
```

```{r plot_isimip_dt_future_merged, dependson="arrange_isimip_dt_future", fig.height=4}

# PLOT ISIMIP MERGED ###########################################################

a <- isimip.future.dt[, sum(V1, na.rm = TRUE), .(year, Continent, model, climate)] %>%
  ggplot(., aes(year, V1, group = interaction(climate, model), color = model)) +
  facet_wrap(~Continent, scales = "free_y", ncol = 5) +
  geom_line() + 
  scale_color_manual(name = "", values = wes_palette(name = selected.palette)) +
  labs(x = "Year", y = bquote("IWW (km"^3 * ")"))  +
  scale_x_continuous(breaks = breaks_pretty(n = 3)) +
  theme_AP() +
  theme(legend.position = "top")

b <- isimip.future.dt[, sum(V1, na.rm = TRUE), .(year, Continent, model, climate)] %>%
  ggplot(., aes(year, V1, group = interaction(climate, model), color = climate)) +
  facet_wrap(~Continent, scales = "free_y", ncol = 5) +
  geom_line() + 
  labs(x = "Year", y = bquote("IWW (km"^3 * ")"))  +
  scale_x_continuous(breaks = breaks_pretty(n = 3)) +
  theme_AP() +
  theme(legend.position = "top")

plot_grid(a, b, ncol = 1, labels = "auto")
```

## ANOVA

```{r anova_isimip, dependson=c("arrange_isimip_data", "arrange_isimip_dt_future")}

# ANOVA ########################################################################

# Arrange ISIMIP datasets ------------------------------------------------------

isimip.full <- isimip.dt[social == "varsoc"][, context:= "historic"] %>%
  rbind(., isimip.future.dt[, context:= "prediction"], fill = TRUE) %>%
  .[, social:= NULL] 

isimip.anova <- isimip.full[, .(estimation = sum(V1)), 
                            .(Continent, climate, context, model, year)]

# ARRANGE DATA #################################################################

columns_to_factor <- c("Continent", "climate", "model")
isimip.full[, (columns_to_factor):= lapply(.SD, as.factor), .SDcols = (columns_to_factor)]
isimip.anova[, (columns_to_factor):= lapply(.SD, as.factor), .SDcols = (columns_to_factor)]

# RUN MODEL AND ANALYSIS OF VARIANCE ###########################################

# List of models ---------------------------------------------------------------

functions <- list(lmm = lmm_fun,
                  gamm = gamm_fun,
                  rf = rf_fun, 
                  bayes = bayes_fun)

# Apply each function to the data and combine results ---------------------------

results <- mclapply(names(functions), function(fun_name) {
  
  isimip.anova[, functions[[fun_name]](.SD), .(Continent, context)]
  
}, 
mc.cores = detectCores() * 0.75)
```

```{r plot_anova, dependson="anova_isimip", fig.height=3.2}

# PLOT RESULTS ##################################################################

results 

results.dt <- rbindlist(results)

a <- isimip.full[, .(estimation = sum(V1)), .(model, Continent, climate, year, context)] %>%
  ggplot(., aes(year, estimation, color = model, group = interaction(climate, model))) +
  geom_line() +
  facet_wrap(context~Continent, scale = "free", ncol = 5) +
  scale_x_continuous(breaks = breaks_pretty(n = 3)) +
  theme_AP() +
  guides(colour = guide_legend(nrow = 2)) +
  labs(x = "Year", y = bquote("IWW (km"^3 * ")"))  +
  theme(legend.position = "top", 
        legend.box.spacing = unit(0, "pt"))

b <- results.dt %>%
  melt(., measure.vars = c("climate_variance", "model_variance", "random_variance", 
                           "residual_variance")) %>%
  .[, .(min = min(value, na.rm = TRUE), 
        max = max(value, na.rm = TRUE)), .(Continent, context, variable)] %>%
  .[, variance:= tstrsplit(variable, "_", fixed = TRUE)[[1]]] %>%
  ggplot(., aes(x = Continent, ymin = min, ymax = max, y = (min + max) / 2, color = variance)) +
  geom_errorbar(width = 0.2) +
  geom_point(size = 1) +
  scale_color_manual(name = "", values=wes_palette(selected.palette, n = 4)) +
  labs(x = "", y = "Fraction variance") +
  facet_wrap(~context, ncol = 1) +
  theme(legend.position = "top") +
  scale_y_continuous(breaks = breaks_pretty(n = 3)) +
  theme_AP() +
  theme(legend.position = "top") + 
  guides(color = guide_legend(nrow = 2)) +
  theme(legend.position = "top") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
   
plot_grid(a, b, ncol = 2, labels = "auto", rel_widths = c(0.72, 0.28))
```

```{r check_combinations, dependson="anova_isimip", fig.height=2.3, fig.width=2.7}

# COUNT COMBINATIONS OF MODEL AND CLIMATE #######################################

unique(isimip.full[, .(model, climate, context)]) %>%
  ggplot(., aes(x = model, y = climate, fill = context)) +
  geom_tile(color = "white", size = 0.5) +
  scale_fill_manual(values = c("historic" = "steelblue", "prediction" = "orange")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Model", y = "Climate", fill = "Context") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  theme_AP() +
  theme(legend.position = "top")
```

# Khan et al dataset

```{r khan_data, cache.lazy=FALSE, eval = FALSE}

# KHAN ET AL 2023 DATASET ######################################################

path.projections <- "./files/khan_et_al_2023"
list.of.files <- list.files(path.projections, pattern = "\\.csv$")
combinations <- lapply(list.of.files, function(x) strsplit(x, "_")[[1]][1:4]) %>%
  do.call(rbind, .) %>%
  data.frame()
colnames(combinations) <- c("SSP", "RCP", "Climate", "Use")

# READ FILES IN PARALLEL #######################################################

# Create parallel cluste -------------------------------------------------------

numCores <- detectCores() * 0.75
cl <- makeCluster(numCores)
registerDoParallel(cl)

# Run for loop -----------------------------------------------------------------

result <- foreach(i = 1:length(list.of.files), 
                  .combine = "rbind",
                  .packages = c("data.table", "countrycode", 
                                "sp", "rworldmap")) %dopar% {
                                  
                                  out <- fread(paste("./files/khan_et_al_2023/", list.of.files[i], sep = "/"))
                                  out[, `:=`(SSP = combinations[i, 1], 
                                             RCP = combinations[i, 2], 
                                             Climate = combinations[i, 3], 
                                             Use = combinations[i, 4])]
                                  
                                  Country <- coords2country(out[1:nrow(out), 2:3])
                                  
                                  df <- cbind(Country, out)
                                  
                                  df[, Continent := countrycode(Country, origin = "country.name", destination = "continent")]
                                  
                                  df[, Dataset := list.of.files[i]]
                                  
                                  df
                                }

# Stop the cluster after the computation ---------------------------------------

stopCluster(cl)
```

```{r arrange_khan_data, dependson="khan_data", cache.lazy=FALSE, eval = FALSE}

# ARRANGE DATA #################################################################

numeric_cols <- grep("^[0-9]+$", names(result), value = TRUE)
khan.dt <- melt(result, measure.vars = numeric_cols, variable.name = "Year") %>%
  .[, Year:= as.numeric(as.character(Year))] %>%
  .[, model:= "GCAM"] %>%
  na.omit() 

# EXPORT DATA ###################################################################

khan.dt.continent <- khan.dt[, .(estimation = sum(value)), 
                             .(Year, Continent, Use, RCP, SSP, Climate, Dataset, model)] %>%
  .[, climate:= paste(Climate, RCP, SSP, sep = "_")] 

fwrite(khan.dt.continent, "khan.dt.continent.csv")
```

```{r plot_khan_continental, dependson="arrange_khan_data", fig.height=2.3, fig.width=4, eval=FALSE}

# PLOT #########################################################################

# Continental ------------------------------------------------------------------

plot.khan.continental <- khan.dt.continent %>%
  ggplot(., aes(Year, estimation, color = Continent, group = interaction(Dataset, Continent))) +
  geom_line(alpha = 0.3) + 
  facet_wrap(~Use) + 
  theme_AP() + 
  theme(legend.position = "top") +
  labs(x = "", y = bquote("km"^3))

plot.khan.continental
```

```{r plot_khan_global, dependson="arrange_khan_data", fig.height=2.3, fig.width=4, eval = FALSE}

# PLOT #########################################################################

# Global -----------------------------------------------------------------------

plot.khan.global <- khan.dt[, sum(value), .(Year, Use, Dataset)] %>%
  ggplot(., aes(Year, V1, group = Dataset)) +
  geom_line(alpha = 0.3) + 
  facet_wrap(~Use) +
  theme_AP() +
  theme(legend.position = "top") +
  labs(x = "Year", y = bquote("km"^3))

plot.khan.global
```

```{r plot_khan_merged, dependson=c("plot_khan_continental", "plot_khan_global"), fig.height=3.5, fig.width=4, eval = FALSE}

# MERGE KHAN ET AL DATASETS ####################################################

plot_grid(plot.khan.continental, plot.khan.global, ncol = 1, labels = "auto", 
          rel_heights = c(0.53, 0.47))

```

```{r plot_khan_ssp_rcp, dependson="arrange_khan_data", eval = FALSE}

# PLOT SSPS VS RCPS ############################################################

khan.dt[, sum(value), .(Year, Use, Dataset, RCP, SSP)] %>%
  ggplot(., aes(Year, V1, group = Dataset, color = Use)) +
  geom_line() + 
  facet_grid(RCP~SSP) +
  theme_AP() +
  theme(legend.position = "top") +
  labs(x = "Year", y = bquote("km"^3))

```

```{r merge_khan_isimip, dependson="anova_isimip", fig.height=1.7, fig.width=6.5}

# MERGE KHAN ET AL DATA WITH ISIMIP ############################################

# Arrange data -----------------------------------------------------------------

khan.dt.continent <- fread("khan.dt.continent.csv")

khan.dt2 <- khan.dt.continent[Use == "withdrawals", .(model, Continent, climate, Year, estimation)] %>%
  setnames(., "Year", "year")

# Extract prediction data from ISIMIP ------------------------------------------

isimip.full2 <- isimip.full[context == "prediction" & year >= 2010, 
            .(estimation = sum(V1)), .(model, Continent, climate, year, context)] %>%
  .[, context:= NULL]

# Merge and plot ---------------------------------------------------------------

merged.dt <- rbind(khan.dt2, isimip.full2) 

ggplot(merged.dt, aes(year, estimation, group = interaction(climate, model), color = model)) +
  geom_line(alpha = 0.4) + 
  facet_wrap(~Continent, scale = "free_y", ncol = 5) +
  theme_AP() +
  scale_x_continuous(breaks = breaks_pretty(n = 3)) +
  theme(legend.position = "top") +
  labs(x = "Year", y = bquote("km"^3))

# Calculate the min and max in 2030-2050 given uncertainty and the global level -----

merged.dt[year %in% c(2030, 2040, 2050), 
          .(min = min(estimation), max = max(estimation)), .(Continent, year)] %>%
  .[, .(sum_min = sum(min), sum_max = sum(max)), year]
```

# Bibliographical study

```{r naomi_data}

# NAOMI DATASET #################################################################

references.projected <- data.table(read.xlsx("./data/references_projection.xlsx")) %>%
  .[, focus:= "projected"]

references.current <- data.table(read.xlsx("./data/references_current.xlsx")) %>%
  .[, focus:= "current"]

references.full.dt <- rbind(references.projected, references.current) %>%
  .[, study:= paste(author, model, climate.scenario, sep = ".")] 

# CLEAN THE DATASET ############################################################

colnames_vector <- c("title", "author", "region")

# Remove leading and trailing spaces -------------------------------------------

references.full.dt[, (colnames_vector):= lapply(.SD, trimws), .SDcols = (colnames_vector)]
references.full.dt[, (colnames_vector):= lapply(.SD, str_squish), .SDcols = (colnames_vector)]

# Lowercaps --------------------------------------------------------------------

references.full.dt[, (colnames_vector):= lapply(.SD, tolower), .SDcols = (colnames_vector)]

# Remove multiple spaces -------------------------------------------------------

references.full.dt[, (colnames_vector):= lapply(.SD, function(x) 
  gsub("\\s+", " ", x)), .SDcols = (colnames_vector)]

# Correct America --------------------------------------------------------------

references.full.dt[, region:= ifelse(region == "america", "americas", region)]

# Extract the publication year -------------------------------------------------

references.full.dt[, publication.date:= str_extract(author, "\\d{4}")] %>%
  .[, publication.date:= as.numeric(publication.date)]
```

```{r naomi_features, dependson="naomi_data", fig.height=1.8, fig.width=2}

# FEATURES OF THE DATASET ######################################################

# Name of different studies ----------------------------------------------------

sort(unique(references.full.dt[variable == "iww" & region == "global", title]))

# Number of data points --------------------------------------------------------

nrow(references.full.dt[variable == "iww" & region == "global"])

# Number of different studies per variable ---------------------------------------

references.full.dt[region == "global", unique(title), variable] %>%
  .[, .N, variable]

# Number of data points for 2000, 2050, 2070, 2100 -----------------------------

references.full.dt[variable == "iww" & region == "global" & 
                     estimation.year %in% c(2000, 2050, 2070, 2100), .N, estimation.year]

# Number of unique studies estimating for 2000, 2050, 2070, 2100 ---------------

references.full.dt[variable == "iww" & region == "global" & 
                     estimation.year %in% c(2000, 2050, 2070, 2100), unique(title), estimation.year] %>%
  .[, .N, estimation.year]

# Number of data points for every targeted year -----------------------------

references.full.dt[variable == "iww" & region == "global", .N, estimation.year] %>%
  .[order(estimation.year)]

# Cumulative sum of published studies ------------------------------------------

cumulative.iww <- references.full.dt[, .(title, publication.date, variable)] %>%
  .[variable == "iww"] %>%
  .[!duplicated(.)] %>%
  setorder(., publication.date) %>%
  .[, .N, publication.date] %>%
  .[, cumulative_sum := cumsum(N)] %>%
  ggplot(., aes(publication.date, cumulative_sum)) +
  geom_line() + 
  scale_x_continuous(breaks = breaks_pretty(n = 3)) +
  geom_point(size = 0.7) + 
  theme_AP() + 
  labs(x = "Year", y = "Nº studies")

cumulative.iww
```

```{r plot_naomi, dependson="naomi_data", fig.height=3.5, fig.width=6}

# PLOT ALL ESTIMATIONS #########################################################

def.alpha <- 0.2

plot.iww <- references.full.dt[variable == "iww" & region == "global"] %>%
  .[, .(author, study, estimation.year, value)] %>%
  na.omit() %>%
  ggplot(., aes(estimation.year, value, color = author, group = study)) +
  geom_point(alpha = def.alpha, size = 0.5) +
  labs(x = "Year", y = bquote("Km"^3)) +
  scale_color_discrete(name = "") +
  geom_line(alpha = def.alpha) +
  theme_AP()

plot.iww

references.full.dt[variable == "iwc" & region == "global"] %>%
  .[, .(author, study, estimation.year, value)] %>%
  na.omit() %>%
  ggplot(., aes(estimation.year, value, color = author, group = study)) +
  geom_point(alpha = def.alpha, size = 0.2) +
  labs(x = "Year", y = bquote("Km"^3)) +
  scale_color_discrete(name = "") +
  geom_line(alpha = def.alpha) +
  theme_AP()

```

## The garden of forking paths

```{r forking_paths, dependson="naomi_data"}

# DEFINE THE UNCERTAINTY SPACE ##################################################

# Target year ------------------------------------------------------------------

target_year <- c(2000, 2050, 2070, 2100)

# Target year interval ---------------------------------------------------------

target_year_interval <- c("yes", "no")

# Interval publication ---------------------------------------------------------

interval <- c(10, 15, 20)

# Metrics of study -------------------------------------------------------------

metrics <- c("cv", "range", "sd", "var", "entropy", "iqr")

# Inclusion criteria -----------------------------------------------------------

inclusion_criteria <- c("all", "exclude_before_1990")

# Rolling windows --------------------------------------------------------------

rolling_window_factor <- c(1, 0.5)

# Define the forking paths -----------------------------------------------------

forking_paths <- expand.grid(target_year = target_year,
                             target_year_interval = target_year_interval,
                             interval = interval,
                             inclusion_criteria = inclusion_criteria,
                             rolling_window_factor = rolling_window_factor,
                             metric = c(metrics, paste(metrics, "_normalized", sep = ""))) %>%
  data.table()

# Number of simulations --------------------------------------------------------

nrow(forking_paths)

# RUN MODEL #####################################################################

trend <- list()

for (i in 1:nrow(forking_paths)) {
  
  trend[[i]] <- forking_paths_fun(dt = references.full.dt,
                                  target_year = forking_paths[[i, "target_year"]], 
                                  target_year_interval = forking_paths[[i, "target_year_interval"]],
                                  interval = forking_paths[[i, "interval"]], 
                                  rolling_window_factor = forking_paths[[i, "rolling_window_factor"]],
                                  inclusion_criteria = forking_paths[[i, "inclusion_criteria"]],
                                  metric = forking_paths[[i, "metric"]])
}
```

```{r naomi_arrange, dependson="forking_paths"}

# ARRANGE DATA ##################################################################

output.dt <- lapply(trend, function(x) x[["results"]]) %>%
  do.call(rbind, .) %>%
  data.table() %>%
  setnames(., "V1", "trend")

final.dt <- cbind(forking_paths, output.dt)

# Print the fraction of simulations in each classification ---------------------

final.dt %>%
  .[, .(total = .N), trend] %>%
  .[, fraction:= total / nrow(output.dt)] %>%
  print()


# Now remove all simulations that produced just one single point ---------------

final.dt <- final.dt[!trend == "single point"]

# Simulations that did not lead to a reduction in uncertainty ------------------

final.dt %>%
  .[, .(total = .N), trend] %>%
  .[, fraction:= total / nrow(output.dt)] %>%
  .[!trend == "Descending"] %>%
  .[, sum(fraction)]
```

```{r examples_plots, dependson="forking_paths", fig.height=3.5, fig.width=3.5, warning=FALSE}

# PLOTS FORKING PATHS EXAMPLES #################################################

plots.dt <- lapply(trend, function(x) x[["plot"]]) 

random.plots <- c(1, 986, 345)
decreasing.plots <- c(1093, 556, 4)
increasing.plots <- c(10, 602, 770)

out.random <- out.decreasing <- out.increasing <- list()

for (i in 1:length(random.plots)) {
  
  out.random[[i]] <- plot_plots_forking_paths_fun(random.plots[i])
  out.decreasing[[i]] <- plot_plots_forking_paths_fun(decreasing.plots[i])
  out.increasing[[i]] <- plot_plots_forking_paths_fun(increasing.plots[i])
}

pt.random <- plot_grid(out.random[[1]] + geom_smooth() + labs(x = "", y = "+ Uncertainty"), 
                       out.random[[2]] + geom_smooth() + labs(x = "", y = ""), 
                       out.random[[3]] + geom_smooth() + labs(x = "", y = ""), 
                       ncol = 3)

pt.decreasing <- plot_grid(out.decreasing[[1]] + geom_smooth() + labs(x = "", y = "+ Uncertainty"), 
                           out.decreasing[[2]] + geom_smooth() + labs(x = "", y = ""), 
                           out.decreasing[[3]] + geom_smooth(method = "lm", se = F) + labs(x = "", y = ""), 
                           ncol = 3)

pt.increasing <- plot_grid(out.increasing[[1]] + geom_smooth(method = "lm", se = F), 
                           out.increasing[[2]] + geom_smooth() + labs(x = "Publication year", y = ""), 
                           out.increasing[[3]] + geom_smooth() + labs(x = "Publication year", y = ""), 
                           ncol = 3)

plot_grid(pt.random, pt.decreasing, pt.increasing, ncol = 1)
```

```{r plot_results_forking_paths, dependson=c("naomi_arrange", "forking_paths"), fig.height=2.2, fig.width=2.2}

# PLOT RESULTS #################################################################

selected_colors <- c("Ascending" = "red", "Descending" = "darkgreen", "Random" = "orange")

plot.fraction <- final.dt[, .(total = .N), trend] %>%
  .[, fraction:= total / nrow(output.dt)] %>%
  ggplot(., aes(trend, fraction, fill = trend)) +
  geom_bar(stat = "identity") +
  labs(x = "", y = "Fraction simulations") +
  scale_fill_manual(values = selected_colors, name = "Uncertainty") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  theme_AP() + 
  theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank(), 
        legend.position = c(0.33, 0.77))

plot.fraction 
```

```{r random_forest, dependson=c("naomi_arrange", "forking_paths"), fig.width=3.5, fig.height=2}

# RANDOM FOREST ################################################################

# Convert categorical variables to factors -------------------------------------

df <- data.frame(final.dt)
df$inclusion_criteria <- as.factor(final.dt$inclusion_criteria)
df$metric <- as.factor(final.dt$metric)
df$trend <- as.factor(df$trend)
df$target_year_interval <- as.factor(df$target_year_interval)

# Train the model --------------------------------------------------------------

rf_model <- randomForest(trend ~ target_year + target_year_interval + interval + 
                           inclusion_criteria + rolling_window_factor + metric, 
                         data = df, importance = TRUE)

# View variable importance -----------------------------------------------------

dt_rf_model <- data.frame(importance(rf_model))
dt_rf_model

# Plot -------------------------------------------------------------------------

plot.rf <- dt_rf_model %>%
  rownames_to_column(., var = "factors") %>%
  data.table() %>%
  setnames(., c("MeanDecreaseAccuracy", "MeanDecreaseGini"), 
           c("Accuracy", "Gini")) %>%
  melt(., measure.vars = c("Accuracy", "Gini")) %>%
  ggplot(., aes(reorder(factors, value), value)) +
  geom_point() +
  coord_flip() +
  facet_wrap(~variable) + 
  scale_y_continuous(breaks = breaks_pretty(n = 3)) +
  labs(x = "", y = "Mean decrease") +
  theme_AP()

plot.rf
```

```{r merge_fraction_rf, dependson=c("random_forest", "plot_results_forking_paths", "forking_paths"), fig.height=2.2, fig.width=6.3}

bottom <- plot_grid(cumulative.iww, plot.fraction, plot.rf, ncol = 3, labels = c("b", "c", "d"), 
          rel_widths = c(0.26, 0.3, 0.44))

bottom
```

```{r merge_fraction_trend, dependson=c("merge_fraction_rf", "plot_naomi", "forking_paths"), fig.height=5.8, fig.width=6}

# 
final.faceted.plot <- plot_grid(plot.iww, bottom, ncol = 1, labels = c("a", ""), 
                                rel_heights = c(0.55, 0.45))

final.faceted.plot

```

```{r plot_forking_paths_faceted, dependson=c("naomi_arrange", "forking_paths"), fig.height=4, fig.width=4}

# RESULTS FACETED BY INTERVAL AND TARGET YEAR, X AXIS METRICS ###################

plot.faceted.metrics <- final.dt %>%
  ggplot(., aes(x = factor(metric), fill = trend)) +
  geom_bar(position ="identity") +
  facet_grid(target_year ~ interval, scales = "free_y") +
  scale_fill_manual(values = selected_colors, name = "Uncertainty") +
  theme_AP() +
  labs(x = "Metric", y = "Nº simulations") +
  theme(legend.position = "none") +
  coord_flip()

plot.faceted.metrics
```


```{r final_final_merged, dependson = c("plot_naomi", "merge_fraction_rf", "random_forest", "forking_paths"), fig.height=7, fig.width=6.5}

bottom <- plot_grid(cumulative.iww, plot.fraction, ncol = 2, rel_widths = c(0.4, 0.6), 
                    labels = c("b", "c"))
left <- plot_grid(bottom, plot.rf, ncol = 1, labels = c("", "d"), rel_heights = c(0.6, 0.4))
bottom2 <- plot_grid(left, plot.faceted.metrics, ncol = 2, labels = c("", "e"))
plot_grid(plot.iww, bottom2, rel_heights = c(0.42, 0.58), ncol = 1, labels = c("a", ""))
```

\newpage

# Session information

```{r session_information}

# SESSION INFORMATION ##########################################################

sessionInfo()

## Return the machine CPU ------------------------------------------------------

cat("Machine:     "); print(get_cpu()$model_name)

## Return number of true cores -------------------------------------------------

cat("Num cores:   "); print(detectCores(logical = FALSE))

## Return number of threads ---------------------------------------------------

cat("Num threads: "); print(detectCores(logical = FALSE))
```